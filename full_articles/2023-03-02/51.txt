像 ChatGPT 这样的大型语言模型是合理性的提供者。 许多聊天机器人基于所谓的生成式人工智能，经过训练可以通过在互联网上搜索相关信息并收集连贯的答案来回答用户的问题，从而生成令人信服的学生论文、权威的法律文件和可信的新闻报道。

但是，由于公开可用的数据包含错误信息和虚假信息，一些机器生成的文本可能不准确或不真实。 这引发了人们争相开发工具来识别文本是由人还是机器起草的。 科学也在努力适应这个新时代，现场讨论是否应该允许聊天机器人撰写科学论文，甚至产生新的假说。

区分人工智能和人类智能的重要性与日俱增。 本月，瑞银分析师透露，ChatGPT 是历史上增长最快的网络应用程序，1 月份每月活跃用户达到 1 亿。 一些部门已经决定没有必要锁住稳定的大门：周一，国际文凭组织表示，学生将被允许使用 ChatGPT 撰写论文，前提是他们注明引用了它。

平心而论，这项技术的创造者坦率地指出了它的局限性。 OpenAI 首席执行官萨姆奥特曼 (Sam Altman) 去年 12 月警告说，ChatGPT「在某些方面足够出色，足以给人一种伟大的误导印象。 我们在稳健性和真实性方面还有很多工作要做。」该公司正在为其内容输出开发加密水印，这是一种机器可读的秘密标点符号、拼写和词序序列； 并且正在磨练一个「分类器」来区分合成文本和人工生成的文本，使用两者的例子来训练它。

斯坦福大学的研究生 Eric Mitchell 认为分类器需要大量的训练数据。 他与同事一起提出了 DetectGPT，这是一种发现差异的「零样本」方法，这意味着该方法不需要事先学习。 相反，该方法会自行启动聊天机器人，以嗅探出自己的输出。

它是这样工作的：DetectGPT 询问聊天机器人它对示例文本的「喜欢」程度，「喜欢」是示例与其自己的创作有多相似的简写。 DetectGPT 然后更进一步——它「扰乱」了文本，稍微改变了措辞。 假设是聊天机器人在「喜欢」更改后的人工生成文本方面比更改后的机器文本更具可变性。 研究人员声称，在早期测试中，该方法在 95% 的时间内正确地区分了人类和机器作者。

需要注意的是：这些结果尚未经过同行评审； 该方法虽然优于随机猜测，但在所有生成式 AI 模型中的工作可靠性并不相同。 对合成文本进行人为调整可能会愚弄 DetectGPT。

这一切对科学意味着什么？ 科学出版是研究的命脉，将想法、假设、论据和证据注入全球科学经典。 一些人很快就将 ChatGPT 作为研究助理，一些有争议的论文将 AI 列为合著者。

Meta 甚至推出了一款名为 Galactica 的科学专用文本生成器。 三天后它被撤回了。 在它被使用的这段时间，构建了一段熊在太空旅行的虚构历史。

Tübingen 的马克斯普朗克智能系统研究所的迈克尔·布莱克教授当时在推特上表示，他对 Galactica 对有关他自己研究领域的多项询问的回答感到「困扰」，包括将虚假论文归咎于真正的研究人员。 「在所有情况下，[Galactica] 都是错误的或有偏见的，但听起来是正确和权威的。 我认为这很危险。」

危险来自于看似合理的文本滑入真正的科学提交，用虚假引用充斥文献并永远歪曲经典。 《科学》杂志现在完全禁止生成文本； 《自然》杂志则允许使用它，前提是必须对使用进行了声明，但禁止将其列为共同作者。

话又说回来，大多数人不会查阅高端期刊来指导他们的科学思维。 如果狡猾的人如此倾向，这些聊天机器人可以按需喷出大量引用伪科学，解释为什么疫苗不起作用，或者为什么全球变暖是个骗局。 在线发布的误导性材料可能会被未来的生成人工智能吞噬，产生新的谎言迭代，进一步污染公共话语。

贩卖怀疑的商人肯定已经摩拳擦掌，迫不及待了。

