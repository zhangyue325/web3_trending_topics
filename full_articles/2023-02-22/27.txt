![](https://img.foresightnews.pro/202302/d22ebc9391799442fe8ad809debac84d.jpeg?x-oss-process=style/scale70)

流行的技术名词按发音难度排序，ChatGPT 肯定排在前面。

到底它为什么叫做 ChatGPT 呢？‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍

先说 GPT：Generative Pre-Training Transformer

**Generative 生成式**

虽然我们已经习惯了话唠的机器人絮絮叨叨的说个不停，但这只是众多的人工智能模型的一种方式。比如还有识别类的（Congnition）：人脸识别，车牌识别这些，还有语音识别，文字识别各种识别任务。（在提到模型的时候，也常常被叫做判别模型，discriminative）。Generative 这个大的种类里面有几个小分支，DALLE 的画图的用的是对抗网络方式 GAN （这个晚些可以分析），现在最火的 Stable Diffusion， MidJourney 走向了另外一个分支，叫做 Difusion，而 ChatGPT 又是一个分支，就是转换器 Transformer。

而 Transformer Generative 的语言模型的核心，通俗的说就是「顺口溜」。

当看了足够多的文本以后，发现有一些语言模式是反复出现的。它之所以可以准确的填补「锄禾日当\_\_ 」的空格，不是因为它在自己的大脑子里面重构了一副农民劳动的场景，仅仅是不过脑子，顺口溜出来的。

你问它： 3457 \* 43216 = ，它回答 149575912 （这是错的。正确结果是 149397712）。之所以结果的 2 是对的，仅仅因为它读了太多的文字资料以后，隐约感觉到

7 结尾的文字，乘号，6 结尾的文字，和 2 结尾的文字比较「押韵」从语感上比较像一首诗，所以它就学会了这样的文字，而不是学会了计算。‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍

生成式模型努力解决的问题，就是给定一些字，预测如果是人类会写什么字。

在 BERT 那个年代，为了训练，大家常常把一句话中随机几个单词遮起来，让计算机用现有的模型预测那几个单词，如果预测准了，就继续加强，如果预测错了，就调整模型，直到上百万上亿次训练之后越来越准。只不过 ChatGPT 的 Generative 的部分，不仅仅把文字，还把上下文、intention（意图）也放进去做训练和预测。‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍

**Pre-Training 预训练**

以前很多的人工智能模型都是为了一个目标训练的。比如给我 1000 张猫的照片，我就很容易的可以训练出来一个模型，判断一个图片是有猫还是没有猫。这些都是专用的模型。

而 Pre-Training 模型不是为了特定的目标训练，而是预先训练一个通用的模型。如果我有特定的需求，我可以在这个基础上进行第二次训练，基于原来已经预训练的模型，进行微调（Fine- Tuning）。

这事儿就像家里请了个阿姨，她已经被劳务公司预训练了整理家务的知识，在此之前已经被小学老师预训练了中文对话，到了我家里面我只要稍微 fine tune 一些我家里特定的要求就好了，而不需要给我一个「空白」的人，让我从教汉语开始把她彻底教一遍才能让她干活。

ChatGPT 的预训练就是给了我们所有人（尤其是创业者，程序员）一个预先训练好的模型。这个模型里面语言是强项，它提供的内容无论多么的胡说八道，至少我们必须承认它的行文通畅程度无可挑剔。这就是他 pre-training 的部分，而回答的内容部分，正是我们需要 fine tuning 的。我们不能买了个 Apache 服务器回来，不灌内容，就说他输出的内容不够呀。‍‍‍‍‍‍‍

**Transformer 转换器**

变电器就是一种 transformer：220 伏电进，12 伏出。

语言的转换器就是把语言的序列作为输入，然后用一个叫做编码器 encoder 的东西变成数字的表现（比如 GPT 就用 1536 个浮点数（也叫 1536 维向量）表示任何的单词，或者句子，段落，篇章等），然后经过转化，变成一串新的序列，最后再用 decoder 把它输出。这个转换器，是这个自然语言处理的核心。

比如如果给 ChatGPT 输入「Apple」这个词，它给你返回

* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 


```
[ 0.0077999732, -0.02301609, -0.007416143, -0.027813964, -0.0045648348, 0.012954261,.....0.021905724, -0.012022103, -0.013550568, -0.01565478, 0.006107009]
```
这 1536 个浮点数字来表示 Apple（其中一个或着多个维度的组合表达了「甜」的含义，另外一堆表达了「圆」的含义，还有一大堆共同表达了「红」等等属性组合，至于具体哪些表达了这些，不得而知）‍‍‍‍‍‍‍‍‍‍‍然后这堆数字，再交给 decoder，并且限定中文的话，它会解码成为「苹果」，限定西班牙语的话，它会解码成「manzana」，限定为 emoji 的话，就输出「🍎」。总之，通过编码，转换，解码，它就完成了从 Apple 到目标输出语言的转化。‍‍‍‍‍‍‍‍‍ChatGPT 所做的事情远远多于翻译。但核心上，它就是把一个语言序列，转换为了另外一堆语言序列，这个任务完成得如此的好，以至于让人产生了它有思想的错觉。‍‍‍‍**GPT 生成式预训练转化器**把上面三段话加在一起，GPT 就是一个预先训练好的，用生成的方式，把输入文字转化成输出文字的翻译‍‍‍

除了这个以外的各种能力和各种定义，大多数是这个翻译官的应用场景而不是它本身。‍‍

**ChatGPT 是啥？**

刚才解释了 GPT，那 ChatGPT 呢？

OpenAI 用这个名字描述他们正在做的这个模型，历史的版本包括 GPT-1， GPT-2（这个模型是开源的），GPT-3（这个就是传说中 175B 参数的大模型）。而这些都是生成式的，也就是给的 prompt（提示词），它负责补全（completion）。但是这个东西用于聊天不是很好用，因为本来就不是为聊天这个场景准备的。‍‍‍‍‍‍‍‍‍‍‍‍

所以在 GPT-3 基础上发展出了下一代模型 InstructGPT，专注于让这个模型可以听懂指令。在上面继续发展出了 ChatGPT，就是对于对话场景，比如多轮对话，还有一些安全边界设定等，进行了加强。但这个模型是基于 GPT-3 上面的，可以说严格意义是 GPT-3 模型上面的一个微调（Fine Tuning）的产物。‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍

希望这样梳理可以帮助大家了解这个奇怪的名字背后的思考。从这个角度来说，这是少有的几个准确的描述了它是什么的一个名字（和 [Web3 这个名字](http://mp.weixin.qq.com/s?__biz=MjM5NzI0Mjg0MA==&mid=2652374640&idx=1&sn=b082857d819af15a31ab724b4e81eba1&chksm=bd3054c78a47ddd139907ae07b4d4412a6097935c83db4ef6739053e116929351034ac9a3f71&scene=21#wechat_redirect)产生鲜明对比）‍‍‍‍‍‍

欢迎转发给对 ChatGPT 感兴趣的朋友。欢迎专业人士指正（我其实不懂自然语言处理里面的细节）‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍

