第一章： Hugging Face 的历史
---------------------

Hugging Face 最早是做 Chatbot 的，做的比较，当时大模型还没有出现，和 ChatGPT 没法比。后来 Google 发布了 Bird, Google 当时做了 TensorFlow, 不过社区已经逐步转向 Pytorch. 所以我们就做了 Pytorch 版的 Bird 副线，把 weights 经过一些办法转化 Pytorch 这种方式，并不是我们重新去训练, 结合这个就慢慢形成了 Transformers。Transformer 是一个模型结构，transformers 是我们这个库，涵盖了所有常用的用到 transformer 结构的这些模型。我们说开发者，researcher 可以很容易把新的模型加上来，用户也可以用同样一个接口，用起来有不同种模型，不仅仅是 NLP（自然语言处理）, CV （计算机视觉）很多领域都在用基于 Transformer 这种架构模型。文生图起来之后呢，我们又对扩散模型做了一个类似的库，叫做 diffusers，把扩散模型都收录进来。这是我们库的主要两个产品；另外做的一个产品就是 Hugging Face Hub。Hugging face.co。我分享下屏幕。

![](https://img.foresightnews.pro/202303/7655724d81330ecfbd614284364cf5c1.png?x-oss-process=style/scale70)

和 Github 比较像，有几个板块。

1）模型：有 models, 有各种各样的模型，我们现在已经有 15 万的模型了。大家如果想学习 NLP 模型，或者想玩什么，就可以在左边 filter 找一下。这里是按照 task 分类的。我个人的理解，NLP 现在用不太上，现在 GPT 太强大了，把整个 NLP 领域都吃掉了。但是 audio （音频）, computer vision （计算机视觉）, multimodal （多模型）这块，还是有些不错的模型。Models, 自己训练了什么模型都可以上传，他和 Github 最大的不同，上传模型后，我们可以帮你存大文件。举个例子，这个模型 GPT-2, OpenAI 一个很早的模型，随便一个文件 tflite，有 400 多兆（MB）。Pytorch 这个文件是 500 多兆，Github 无法储存这么大的文件，我们提供的服务包括大文件的存储和大文件的 CDN 服务。模型界面右边有 hosted inference API。你这个模型传上来之后，他会去猜这个模型正在干嘛。看到 GPT-2, 他会知道可能是一个文本生成模型，text generation，会帮你设置 weject...你可以在这与你的模型进行交互。这里的例子就是我的名字是什么，后面的蓝色字体就是生成出来。

2）Datasets: 训练模型的时候用了 datasets, 或者准备了一些，这里比较有意思的 filter 就是 size。我们这里存储最大的模型是什么样的，最大的是 poloclub/diffusiondb，大小是几个 T（size categories 大于 1T)，每个文件都是几百兆，我们提供免费服务。最后一个有意思的模型是 Spaces。最后一个也是我觉得最有意思的是 Spaces，其实就是可以跑的 Github 应用。Github 把代码开放出来之后，具体怎么跑，装什么 dependency 才可以跑，不是特别确定。Spaces 可免费提供 easy to use instance。 亚马逊的虚拟机，你在上面可以直接跑起来。你也可以挂一些 GPU，把大的模型跑起来试下功能。这个 OpenChatKit 是最近比较火的，不过这个机器人有点傻，我之后会给大家介绍更多更强大的机器人。这个机器人主要目的是提供 feedback，你可以给他打分，回复好，不好。这个数据会被汇总，以后再训练 openchatkit 的时候，Open Kit System 就会记录下来。有点像 reinforcement learning from human feedback，把这个评分，下次降低他的有限级别，等等。这个就是 Space 的介绍。

另外给大家介绍一个比较有意思的，如果大家想来 hugging face 玩，注意下 Trending 这个 page。目前很多开源模型都会直接放在 hugging face 上面，可以看到哪些模型比较火，比如我们看模型类别，清华的 chatglm-6b 就比较火，1 万多的下载，200 多的点赞，这里下载量为两周下载量，而不是 Github 上所有的下载量，所以看上去数值比较小。另外 ControlNet, Stable Diffusion, OrangeMixs, anything 我之后会介绍, 非常火的文生图模型。Spaces 是非常有意思的，发布模型后，有很多人会写 UI 界面，我们刚才也看了 OpenChatKit。

![](https://img.foresightnews.pro/202303/b3456ec498df87953e1028cd2f5a5f02.png?x-oss-process=style/scale70)

这个 Spaces 大家会看到有前端，有后端，我和他可能还会有一些交互。比如我可以让他生成图片。这个写起来会不会很费力？我输入 A high tech solarpunk utopia in the Amazon rainforest (low quality) 在亚马逊热带雨林高科技太阳朋克乌多邦（不会）。比如我们看 Stable Diffusion 这个代码, gitattributes，README 这些都不用看。看 app.py，读一下他的代码，其实非常简单。下面是一堆 html，去 render 这个 page。核心的代码就是 infer 函数（15 行）。一旦用户点了 bottom 之后，然后我们就做点事情，就把 image 生成出来。Return image， 然后就在 UI 上展示。这个就是用 gradio 来做 （gradio.app)，如果大家感兴趣可以了解下。核心思想就是说用不了几行代码，就可以写几行前后端分离的页面出来，大大提高了 AI app 的生产力。这就是 Hugging Face 的简单介绍。

第二章 Hugging Face 上有趣的 Space
---------------------------

Spaces 我自己有 awesome Hugging Face Spaces 的 list, 现在已经不够 awesome 了，chatGPT 真的是太牛了。4 出来之后，很多 spaces 距离商业化的产品还差很多，但是还是有些大家会感兴趣。比如说 YOLO, 如果是上一波做 CV 的话，YOLO 是非常常见的做 detection 的模型。核心的思想就是可以把这个里面的东西找出来，比如说让城市摄像头去数一数街道上有多少辆车。可以跑一个 YOLO 模型，帮你找 Bounding Box（候选框）, 帮你把所有的车找出来。这个作者写了一个 Space, 很直观的看出来 YOLOv8 和 v7 或者其他版本有什么不一样的地方。对于文生图领域，可能更是这样。以前可能看下准确率就够了，就能知道这个模型是好是坏，现在对于很多模型来讲，你需要去试一下，看看和你的手感是否比较配合，或者说在一些使用场景上是否能够满足你的需求。这个结果就跑出来了，因为用了 CPU，所以跑起来比较慢，不过效果还是不错的。

第三章：AIGC 和 Demos
----------------

AIGC 和空耳这种现象非常相近，给大家举个例子。当你去听韩语或者印度歌， 第一次听的时候你可能觉得他是噪音，讲什么听不懂，你可能知道他有点韵律，他说什么你不清楚。一旦你看了字幕，这个字幕实际上是有人把他听出来，用中文的，发音非常像的词语描述一下，再去听的时候，你可能觉得这个就是讲的这个歌词，虽然歌词非常无厘头，比如说的我说你别介意不洗澡。一旦你接受了歌词的设定，再去听这首歌的时候，你就会发现他好像唱的就是这个东西，再也不是一个噪声了，而是一个字幕在说的事情。对于 AIGC 讲的是同样的事情。

扩散模型是怎么回事呢，实际上一个降噪的过程。我们想象加噪声是什么样的过程。原来是一张图，一点点加噪声，一点点加噪声，慢慢的图是什么我们就看不清了。和我们刚才听歌一样，一开始是非常清晰的声音，经过麦克风录制一次声音就变差一次，经过我把喇叭放出来，经过我电脑麦克风收进去，再到喇叭放出的时候，可能就是噪音了。Diffuison model 做的是反向的过程，需要把噪音里面的信息提取出来，如何提取呢，如果你完全不给他方向，韩语歌你让他听 100 遍，还是听不懂是什么，永远都是在这个状态，让他往左边走的模式就是我给他一个 prompt, control, imbedding，给他额外的信息，让他去空耳，让他感觉我好像看到的是右数第二张图，就会一点点从最右边的图片往左边移动。这里我好像看到一只猫。其实我看了右数第二张图，再看右数第一张图，我是依稀能看到有个猫出来的。走完第一步，就可以走第二部，最后完全还原出猫。认为杂乱无章的图片是一只猫，他就会一点点看出一只猫。至于原来是什么不重要，韩语唱的是什么不重要，我满脑子就是不洗澡。

![](https://img.foresightnews.pro/202303/ea681eeed2d6baff260b591dc0b655a2.png?x-oss-process=style/scale70)

大家知道 Stable Diffusion 有自己的模型，比如说 Stable Diffusion v1-5, 我们用原始模型去生成一些东西，大家可以去对比下效果。再拿比 anything v4 (Fantasy.ai) 微调过的模型去看下效果。Anything v4 有点动漫风格。不同模型，训练集，训练方法，训练权重，都不一样，风格也不一样。

  


![](https://img.foresightnews.pro/202303/ef9ab25934f938a01208adae83c3ddf0.png?x-oss-process=style/scale70)

![](https://img.foresightnews.pro/202303/4fae17ddb1c1eb4dc9c74aceed981a52.png?x-oss-process=style/scale70)

  


![](https://img.foresightnews.pro/202303/98875912885349bbaac8532f402b8d71.png?x-oss-process=style/scale70)

刚才提到，如何让我的模型学到 concept。比如说标准模型有狗的形态，但是你说画一个狗的图片。但是这个狗和你家的狗还是不一样。每条狗都有独特的地方，通过 DreamBooth 技术, 可以让模型学会这一条狗是什么样子，学会之后，我可以在不同场景生成这个狗的照片，我们可以观察到训练集有不同的狗的角度。

  


![](https://img.foresightnews.pro/202303/2d5c75d86a40fe276a22d348b73b42ff.png?x-oss-process=style/scale70)

DreamBooth 是一个微调技术，讲究用少数几张照片就可以达到这样的效果。如果你有更多的照片去调参数，可能有更好的效果。如果 3-5 张照片，如果能实现不错的效果，其实就是很好的结果了。这个技术最早是 Google 发明的。但是这不是唯一植入芯的 concept 技术，还有 texing 模式等。我们之前围绕这个技术做了比赛。这里是大家上传的 DreamBooth 模型，获奖的是国潮风，他训练的不是具体的东西，训练的是国朝的风格。

  


![](https://img.foresightnews.pro/202303/3d1529a9ca0c2c21f8f8e9d428fd2b95.png?x-oss-process=style/scale70)

模型训练出来就认识这个咒语：尖括号 guo-chao 尖括号，只要你输入，就意识到你要这个风格。DreamBooth 的问题在于这个模型太大了，训练出来 4-5G。所以后面有个新的技术，两个叠加在一起，就可以把微调的模型变得非常小，比如说 LoRA+DreamBooth。

  


![](https://img.foresightnews.pro/202303/f4b32b9c84befb1a9e704426882107a6.png?x-oss-process=style/scale70)

![](https://img.foresightnews.pro/202303/89b20a07e052bb8c20fa69d93350b51d.png?x-oss-process=style/scale70)

我们和飞桨做了一个比赛，百度飞桨提供算力，提供代码。大家只需要准备几张照片，去参加，选一个 GPU。把照片拖到飞桨的计算中心，跑一下，就可以做出这个模型。

这是一个行星发动机, WonderingEarth。我们看一下这个参赛选手，a man looking at the WonderingEarth。 因为是 LoRA, 可以把不同的 concept 串到一起。假象有个人做了 concept 叫做月球轨道车，prompt a man looking at the WonderingEarth+ 月球轨道车，这里就会出现月球轨道车，你可以把不同的 concept 组合到一起，达到效果，甚至你可以说，国朝风格的行星发动机。当然里面有很多技术细节，大体的思路是这样的。

  


![](https://img.foresightnews.pro/202303/6b7e1b3c3a629afd06928ac92435929f.png?x-oss-process=style/scale70)

接下来我讲下我个人比较看好的技术，Elite。刚才我们说我让模型学会一个新的概念，我需要有一个训练的过程。对于传统机器学习，fewshot 这个概念，我有没有可能做到 zeroshot。我给你几张照片，你不需要重新微调，微调需要十分钟二十分钟。你能否看到我这张照片就知道我要做什么，直接画出来。举个例子，我选了小猫这张照片，给了他一个 mask，告诉他小猫的这个位置是我想让他在新图片中生成的。这个示例就默认这个概念叫 S, 之前的概念叫做国潮，或者 Wonderingearth,这里叫做 S。我的指令 S 在一个杯子中的（S in jar）照片。我们看到效果还是不错的。如果用 DreamBooth，和 LoRA, 效果会更好。毕竟训练这么久，又有那么多张照片。这个一张照片就可以生成这样的效果。 

  


![](https://img.foresightnews.pro/202303/533d780aaab6777b2f18f5ef2f7f86c6.png?x-oss-process=style/scale70)

另外一个是 ControlNet, 刚才我们说空耳的时候给你歌词，然后你根据这个歌词去想象歌曲在唱什么。现在我不给你歌词，我给你画，给你其他的 Control, 是不是可以呢，实际上在模型，其实在模型种都是不同的 control，给你算 proseattention。在图像里，你可以给他一个边缘，告诉他，你从这个虚无中看到一个边缘，你能看到什么，然后再额外给你一个 prompt， 然后生成效果。我给他的 Prompt 是一个男孩，有几张照片还是能看出来的，虽然左边是个女孩，但是右边的照片形态上满足了我的要求。他在边缘上也满足了黑白边缘照片的要求。我如果可以画张素描，和他说这个素描画的是什么，他就可以把这个素描填色，修改，光影，都处理好，还可以叠加像 LoRA 这种新技术。把一些 concept 也加进来。

  


![](https://img.foresightnews.pro/202303/212fde837f8af36df5a6a3d1e4ddb74d.png?x-oss-process=style/scale70)

这是另外一种 Control, 刚才我们运用的是边缘，这里是 pose estimation，调用的是另一个 AI 模型去识别，识别骨头关键点，头在哪，手在哪。识别完，再给上 prompt，比如男孩，手没有处理好，头，脚的位置都是不错的。同时给我们生成了四张。这就是 ControlNet, 给他更多 Control, 让他按照你的思路去生成。

  


![](https://img.foresightnews.pro/202303/6fb48cdfecf18784ef79e5cfb06abc3c.png?x-oss-process=style/scale70)

把这个思路往下扩展，可以做更多事情，比如说是否可以把所有 possibilities 全部列出来，我自己去选需要什么。比如说腾讯最近做的 T2-Adapter, 它可以把 Style & Color, Structure, sketch, pose, 深度，边缘全部都用起来。这个例子就是我拿呐喊的图作为 Style, 我要用我的 Pose, 我要画一个和尚。同样手有点诡异。如果我给的全身照，包括手，效果应该会更好。这个有趣的地方你可以把这些东西各种 combination。ControlNet 也可以做，multicontrolNet, 不过 UI 没有这么好，腾讯这个你可以直接选择。

  


![](https://img.foresightnews.pro/202303/ef2e2d1c7512ac9a840b094271d0f373.png?x-oss-process=style/scale70)

阿里有个新的工作就是 Composer。可以从一张图片中拿到 Sketch，色阶,mask 等等。根据不同组合方式，可以生成不同的图片。目前还没有开源。 

  


![](https://img.foresightnews.pro/202303/ae869326cc0b700c2bdcdedd7d3771a2.png?x-oss-process=style/scale70)

接下来介绍一下非模型的，前沿生产力工具。第一个是 Robust Face Restoration 和 upscale 的模式。场景是 Stable diffusion 默认生成 512\*512 的图，用 upscale 可以生成 2048\*2048 的图。这是 upscaler 的其中一种。可以通过这种方式，看效果如何，如果不错，可以再选择慢一点的 upscale，把图片效果做到更好。

刚才上述介绍的都是单一工具，如果大家以这个谋生或者发展兴趣，需要做 Webui UI 这个开源工具，很多人在 Hugging Face 上去部署。提供了很多功能，这只是阉割版本，有些插件没有安装，但是可以做 text 2 image. 给他 prompt，甚至 negative prompt，很多参数可以选择，随后生成了山水画。局部编辑，指示他在某个位置画飞机，他就可以画飞机，云山雾罩中有个飞机。感觉要从画中飞出来一样。还有 image2image, 大家可以研究。

最后给大家看些声音生成的例子。这里可以用原神人物的风格，生成说的话。唱歌的，帮你生成歌曲。改音高，duration，像我只会改歌词，你有不同的歌曲风格可以去换。现在歌曲的风格，或者说人物的声音，都是需要很多数据去训练，未来或许也会出现类似 LoRA 这种技术，我们可以去选，把不同的风格叠加起来，比如说让邓紫棋唱周杰伦的歌，加上我们自己的歌词。或者未来或出现 zeroshot, 类似于 Elite 风格，我给他一首歌，现场就学了这种风格，分解出不同的维度，根据我的需求选择性拼贴，变成新的歌曲。

